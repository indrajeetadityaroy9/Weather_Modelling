{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import re\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy.stats import boxcox, skew\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Overview"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb4ff28d2ba3f84c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('weather_prediction_dataset.csv')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88f775cd0b2e4979"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.columns"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "946586c1244f61b6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cities = ['BASEL', 'BUDAPEST', 'DE_BILT', 'DRESDEN', 'DUSSELDORF', 'HEATHROW', 'KASSEL', \n",
    "          'LJUBLJANA', 'MAASTRICHT', 'MALMO', 'MONTELIMAR', 'MUENCHEN', 'OSLO', 'PERPIGNAN', \n",
    "          'ROMA', 'SONNBLICK', 'STOCKHOLM', 'TOURS']\n",
    "print(\"\\nCities:\")\n",
    "print(sorted(cities))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b4cecbbd798f85f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "city_var_pairs = [col for col in df.columns if any(col.startswith(city) for city in cities)]\n",
    "print(\"\\nCity Variable pairs:\")\n",
    "print(city_var_pairs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e47101a8e0f870e9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "variables = set()\n",
    "city_variable_dict = {}\n",
    "\n",
    "# Extract weather variables based on city names\n",
    "for city in cities:\n",
    "    found_variables = [col.split(city + \"_\")[1] for col in city_var_pairs if col.startswith(city + \"_\")]\n",
    "    variables.update(set(found_variables))\n",
    "    city_variable_dict[city] = found_variables\n",
    "\n",
    "# Convert variables set to sorted list\n",
    "variable_list = sorted(variables)\n",
    "print(\"\\nWeather Variables:\")\n",
    "print(variable_list)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0cc5aae28f64cdd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Cities are rows and variables are columns, populated with 0/1\n",
    "data_matrix = pd.DataFrame(0, index=cities, columns=variable_list)\n",
    "\n",
    "for city, var_list in city_variable_dict.items():\n",
    "    for var in var_list:\n",
    "        data_matrix.loc[city, var] = 1\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(data_matrix, annot=True, cmap=\"Blues\", cbar=False, linewidths=0.5)\n",
    "plt.title('City vs Weather Variables')\n",
    "plt.xlabel('Weather Variables')\n",
    "plt.ylabel('Cities')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "439dcd31cad5f03"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As there are 18 cities, building a generalizable model requires a more compact and scalable feature space.\n",
    "\n",
    "Efficient Feature Space: The number of features remains constant, regardless of the number of cities, simplifying the model and avoiding unnecessary complexity.\n",
    "\n",
    "Improved Generalization: The model generalizes more effectively since the feature structure stays consistent, with the city treated as just one additional feature.\n",
    "\n",
    "Scalability: This approach scales efficiently when handling many cities or expanding the model to include new ones.\n",
    "\n",
    "Categorical Encoding: The city column can easily be encoded as a categorical feature (e.g., using one-hot encoding or embeddings) without significantly increasing the feature space.\n",
    "\n",
    "Compact Feature Space: Instead of generating a separate column for each city-variable combination, you only need one set of columns for weather variables, which significantly reduces dimensionality.\n",
    "\n",
    "Optimized for Model Training: By minimizing dimensionality, this approach mitigates the curse of dimensionality, making the dataset more efficient for training machine learning models. The city can be encoded using techniques like one-hot encoding or embeddings.\n",
    "\n",
    "Flexible and Scalable: This structure is highly adaptable, making it easier to add more cities or extend the model to a longer time range."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f96afc30b653e3e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Reshaping"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3da1e40ddd5004fc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cities = ['BASEL', 'BUDAPEST', 'DE_BILT', 'DRESDEN', 'DUSSELDORF', 'HEATHROW', 'KASSEL',\n",
    "          'LJUBLJANA', 'MAASTRICHT', 'MALMO', 'MONTELIMAR', 'MUENCHEN', 'OSLO', 'PERPIGNAN',\n",
    "          'ROMA', 'SONNBLICK', 'STOCKHOLM', 'TOURS']\n",
    "\n",
    "weather_variables = ['cloud_cover', 'global_radiation', 'humidity', 'precipitation', 'pressure', 'sunshine', 'temp_max', 'temp_mean', 'temp_min', 'wind_gust', 'wind_speed']\n",
    "\n",
    "city_var_pairs = [col for col in df.columns if any(col.startswith(city) for city in cities)]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1612a65974e4343"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_city_and_variable(col_name):\n",
    "    for city in cities:\n",
    "        if col_name.startswith(city):\n",
    "            return city, col_name[len(city)+1:]  # Extract city and variable\n",
    "    return None, None"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5f57cd1f41f1ecf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Reshape the dataframe to long format\n",
    "long_format_df = pd.melt(\n",
    "    df,\n",
    "    id_vars=['DATE', 'MONTH'],  # Keep DATE and MONTH columns as they are\n",
    "    value_vars=city_var_pairs,  # Use city-variable pairs\n",
    "    var_name='city_variable',  # Temporary column for city-variable combined names\n",
    "    value_name='value'  # Column for the values\n",
    ")\n",
    "\n",
    "# Split the 'city_variable' into 'city' and 'variable' using the custom function\n",
    "long_format_df[['city', 'variable']] = long_format_df['city_variable'].apply(lambda x: extract_city_and_variable(x)).apply(pd.Series)\n",
    "\n",
    "# Drop the 'city_variable' column since it's no longer needed\n",
    "long_format_df.drop(columns=['city_variable'], inplace=True)\n",
    "\n",
    "# Pivot the table to get the final reshaped format\n",
    "reshaped_df = long_format_df.pivot_table(index=['DATE', 'MONTH', 'city'], columns='variable', values='value').reset_index()\n",
    "\n",
    "reshaped_df.to_csv('reshaped_weather_prediction_dataset.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8dbd9091b569bb41"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Overview (after reshaping)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7912eacc3ced3d5c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reshaped_df = pd.read_csv('reshaped_weather_prediction_dataset.csv')\n",
    "reshaped_df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61a8015fe0cbf90d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reshaped_df.describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f97a55e6e9facf1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reshaped_df.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f3e381f0c9637b8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reshaped_df.dtypes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9bb0cbbb029271f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Confirming dataset is time-based. Yes, the dataset is time-based as the dates are unique and ordered chronologically  (monotonically increasing) and there are no missing dates"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aad784c786db2344"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reshaped_df['DATE'] = pd.to_datetime(reshaped_df['DATE'], format='%Y%m%d')\n",
    "\n",
    "# Check for unique dates and their order\n",
    "unique_dates = reshaped_df['DATE'].nunique()\n",
    "total_rows = len(reshaped_df)\n",
    "date_order_correct = reshaped_df['DATE'].is_monotonic_increasing\n",
    "\n",
    "# Check for any missing or irregular time intervals\n",
    "date_range = pd.date_range(start=reshaped_df['DATE'].min(), end=reshaped_df['DATE'].max())\n",
    "missing_dates = date_range.difference(reshaped_df['DATE'].unique())\n",
    "\n",
    "unique_dates, total_rows, date_order_correct, len(missing_dates), missing_dates[:10]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fffd37e875c92cdd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Missing Data Analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27a2f3228ddc3cb6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "missing_data = reshaped_df.isnull().sum()\n",
    "missing_data = missing_data[missing_data > 0]\n",
    "missing_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6f32b1ae42018a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "missing_data_proportion = reshaped_df.isnull().mean() * 100\n",
    "missing_data_proportion = missing_data_proportion[missing_data_proportion > 0]\n",
    "missing_data_proportion = missing_data_proportion.sort_values(ascending=False)\n",
    "missing_data_proportion_formatted = missing_data_proportion.apply(lambda x: f\"{x:.2f}%\")\n",
    "missing_data_proportion_formatted"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1722a15795a96060"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Variables with Low Missing Data (<10%): \n",
    "\n",
    "precipitation and temp_min are the least affected by missing data and be left as is, as the low percentage of missing data is unlikely to significantly impact model training. \n",
    "\n",
    "Variables with Moderate Missing Data (10-30%): \n",
    "cloud_cover, global_radiation, humidity, pressure, sunshine, and wind_speed fall are moderately affected by missing data.As the dataset has been confirmed to be time-based, time-based imputation is an effective approach.\n",
    "\n",
    "Forward-fill: The missing values for a variable are filled using the most recent available value (i.e., carry the last known value forward).\n",
    "\n",
    "Backward-fill: If there are missing values at the beginning of a series, backward-fill can be used to propagate future values backward.\n",
    "\n",
    "Variables with High Missing Data (>30%): \n",
    "\n",
    "wind_gust has 61.11% missing data, making it unreliable for use in model training.Given the high percentage of missing data, the variable is dropped from analysis entirely, as attempting to impute such a large portion may introduce bias or noise."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75f6c5f6648ba840"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Drop 'wind_gust' due to high percentage of missing data\n",
    "reshaped_df = reshaped_df.drop(columns=['wind_gust'])\n",
    "\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "constant_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "# Using mean imputation for 'temp_min'\n",
    "reshaped_df['temp_min'] = mean_imputer.fit_transform(reshaped_df[['temp_min']])\n",
    "\n",
    "# Using constant imputation (0) for 'precipitation'\n",
    "reshaped_df['precipitation'] = constant_imputer.fit_transform(reshaped_df[['precipitation']])\n",
    "\n",
    "# Variables with moderate missing data to be imputed\n",
    "variables_to_impute = ['cloud_cover', 'global_radiation', 'humidity', 'pressure', 'sunshine', 'wind_speed']\n",
    "\n",
    "# Apply forward-fill to impute missing values\n",
    "reshaped_df[variables_to_impute] = reshaped_df[variables_to_impute].ffill()\n",
    "\n",
    "# Apply backward-fill to handle any remaining missing values at the start of the series\n",
    "reshaped_df[variables_to_impute] = reshaped_df[variables_to_impute].bfill()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9e29d00e9c92a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reshaped_df.to_csv('reshaped_weather_prediction_dataset.csv', index=False)\n",
    "reshaped_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6dcdd882643a3fb2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "missing_data = reshaped_df.isnull().sum()\n",
    "missing_data = missing_data[missing_data > 0]\n",
    "missing_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95b65e6f1d185cad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reshaped_df['year'] = reshaped_df['DATE'].dt.year\n",
    "reshaped_df['month'] = reshaped_df['DATE'].dt.month\n",
    "reshaped_df['day'] = reshaped_df['DATE'].dt.day\n",
    "reshaped_df.drop(columns=['DATE', 'MONTH'], inplace=True)\n",
    "reshaped_df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6cecb9734a4e211"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reshaped_df.describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed77532cf0fb3f63"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Distribution and Univariate Analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57f4c5c3be4e90ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "categorical_features = reshaped_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numerical_features = reshaped_df.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "print(\"Categorical Features:\")\n",
    "print(categorical_features)\n",
    "\n",
    "print(\"\\nDistinct values for each categorical feature:\")\n",
    "for feature in categorical_features:\n",
    "    distinct_values = reshaped_df[feature].nunique()\n",
    "    unique_values = reshaped_df[feature].unique()\n",
    "    print(f\"{feature}: {distinct_values} unique values\")\n",
    "    print(f\"Values: {unique_values}\")\n",
    "\n",
    "print(\"\\nNumerical Features:\")\n",
    "print(numerical_features)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a3fa62c62a618bb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "numerical_features = ['cloud_cover', 'global_radiation', 'humidity', 'precipitation', \n",
    "                      'pressure', 'sunshine', 'temp_max', 'temp_mean', 'temp_min', \n",
    "                      'wind_speed']\n",
    "\n",
    "num_features = len(numerical_features)\n",
    "cols = 3\n",
    "rows = math.ceil(num_features / cols)\n",
    "\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(18, 6 * rows))\n",
    "axs = axs.flatten()\n",
    " \n",
    "for i, feature in enumerate(numerical_features):\n",
    "    sns.histplot(reshaped_df[feature], bins=20, kde=True, ax=axs[i])\n",
    "    axs[i].set_title(f'{feature.capitalize()} Distribution')\n",
    "    axs[i].set_xlabel(feature.capitalize())\n",
    "    axs[i].set_ylabel('Frequency')\n",
    "\n",
    "for j in range(i + 1, len(axs)):\n",
    "    fig.delaxes(axs[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b8547262e52eb85"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Shape Analysis (Skewness)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "774b9bc67e4ebc4a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "numerical_features = ['cloud_cover', 'global_radiation', 'humidity', 'precipitation', \n",
    "                      'pressure', 'sunshine', 'temp_max', 'temp_mean', 'temp_min', \n",
    "                      'wind_speed']\n",
    "skewness = reshaped_df[numerical_features].skew()\n",
    "print(\"Skewness of features:\\n\", skewness)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1b0c68dc1da5614"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Highly Skewed Features:\n",
    "\t•\tcloud_cover, precipitation, pressure, and wind_speed are highly skewed, which could impact the performance of regression models like Linear Regression. These should be the focus for transformation.\n",
    "\n",
    "Moderate Skewness:\n",
    "\t•\tglobal_radiation, humidity, sunshine, and temp_min may still benefit from some transformation, but their skewness is less extreme."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f7ad505c9b0ed26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "# Cloud Cover: Reflection + Log\n",
    "reshaped_df['cloud_cover'] = np.log1p(-reshaped_df['cloud_cover'] + np.max(reshaped_df['cloud_cover']) + 1)\n",
    "\n",
    "# Global Radiation: Box-Cox\n",
    "reshaped_df['global_radiation'], _ = boxcox(reshaped_df['global_radiation'] + 1)\n",
    "\n",
    "# Humidity: Box-Cox\n",
    "reshaped_df['humidity'], _ = boxcox(reshaped_df['humidity'] + 1)\n",
    "\n",
    "# Precipitation: Box-Cox\n",
    "reshaped_df['precipitation'], _ = boxcox(reshaped_df['precipitation'] + 1)\n",
    "\n",
    "# Pressure: Outlier Removal + Reflection + Log\n",
    "Q1 = reshaped_df['pressure'].quantile(0.25)\n",
    "Q3 = reshaped_df['pressure'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "reshaped_df = reshaped_df[(reshaped_df['pressure'] >= lower_bound) & (reshaped_df['pressure'] <= upper_bound)]\n",
    "reshaped_df['pressure'] = np.log1p(-reshaped_df['pressure'] + np.max(reshaped_df['pressure']) + 1)\n",
    "\n",
    "# Wind Speed: Log\n",
    "reshaped_df['wind_speed'] = np.log1p(reshaped_df['wind_speed'])\n",
    "\n",
    "new_skewness = reshaped_df[numerical_features].skew()\n",
    "print(\"Skewness after transformation:\\n\", new_skewness)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ba9bc42fbf3b4d6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The cloud_cover, global_radiation, humidity, pressure, and wind_speed features are now nearly perfectly normalized.\n",
    "\n",
    "The temperature features are still slightly negatively skewed, but this is acceptable for most modeling scenarios.\n",
    "\n",
    "The precipitation and sunshine features still have some moderate skewness, but these values are much improved and should not cause significant issues in most models."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2887a0ccf51ce89b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cols = 3 \n",
    "num_features = len(numerical_features)\n",
    "rows = (num_features // cols) + 1 if num_features % cols != 0 else num_features // cols\n",
    "plt.figure(figsize=(16, 4 * rows))\n",
    "\n",
    "for i, feature in enumerate(numerical_features, 1):\n",
    "    plt.subplot(rows, cols, i)\n",
    "    sns.kdeplot(reshaped_df[feature], shade=True)\n",
    "    plt.title(f'Distribution of {feature} (KDE)')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be68722f92a88a72"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Outlier Analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "247ce68155f9db54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 12))\n",
    "for i, feature in enumerate(['cloud_cover', 'global_radiation', 'humidity', \n",
    "                             'precipitation', 'pressure', 'sunshine', \n",
    "                             'temp_max', 'temp_mean', 'temp_min', 'wind_speed'], 1):\n",
    "    plt.subplot(5, 2, i)\n",
    "    sns.boxplot(x=reshaped_df[feature])\n",
    "    plt.title(f'Boxplot of {feature}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa48feb2675b8da9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_iqr_bounds(df, column):\n",
    "    # Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Calculate the outlier bounds\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "for feature in ['cloud_cover', 'global_radiation', 'humidity', 'precipitation', \n",
    "                'pressure', 'sunshine', 'temp_max', 'temp_mean', 'temp_min', 'wind_speed']:\n",
    "    lower, upper = calculate_iqr_bounds(reshaped_df, feature)\n",
    "    print(f\"{feature} has bounds - Lower Bound: {lower}, Upper Bound: {upper}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "805b89e5ebba8a1a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "No outlier capping or removal required:\n",
    "\n",
    "Cloud cover usually ranges from 0 (no cloud cover) to around 8 (completely overcast). The outliers detected here might not represent erroneous data but could indicate low cloud cover conditions. Since the upper bound is 2.79, extreme cloud cover values (close to fully overcast) are not flagged as outliers.\n",
    "\n",
    "Humidity is typically expressed as a percentage (0%–100%). Depending on the transformation applied, values near 0.13 may represent very low humidity levels, which could occur in extremely dry climates. Upper values may represent high-humidity conditions, potentially caused by local weather.\n",
    "\n",
    "Outlier capping required:\n",
    "Temperature Features (temp_max, temp_mean, temp_min): These features have many outliers, but they may be valid extremes that represent important weather events and are extreme value capped as they are part of normal seasonal variation or extreme weather.\n",
    "\n",
    "Wind speed typically follows a log-normal distribution, with most values clustering near the lower end and some extreme wind events. Outliers might represent strong winds or storms.\n",
    "\n",
    "Outlier removal:\n",
    "Sunshine is typically measured in hours per day. Negative values don’t make sense in this context and could represent erroneous entries."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1452c01e10f0520"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def cap_outliers(df, column, lower_bound, upper_bound):\n",
    "    df[column] = np.clip(df[column], lower_bound, upper_bound)\n",
    "    return df\n",
    "\n",
    "capping_features = ['temp_max', 'temp_mean', 'temp_min', 'wind_speed']\n",
    "bounds = {\n",
    "    'temp_max': (-12.45, 42.35),\n",
    "    'temp_mean': (-12.95, 34.65),\n",
    "    'temp_min': (-13.1, 26.9),\n",
    "    'wind_speed': (0.32, 2.56)\n",
    "}\n",
    "\n",
    "for feature in capping_features:\n",
    "    lower_bound, upper_bound = bounds[feature]\n",
    "    reshaped_df = cap_outliers(reshaped_df, feature, lower_bound, upper_bound)\n",
    "    \n",
    "reshaped_df = reshaped_df[reshaped_df['sunshine'] >= 0]\n",
    "\n",
    "lower_bound_pressure = 0.693\n",
    "upper_bound_pressure = 0.714\n",
    "reshaped_df = cap_outliers(reshaped_df, 'pressure', lower_bound_pressure, upper_bound_pressure)\n",
    "\n",
    " # No outliers but keeping within these bounds for consistency\n",
    "bounds_other = {\n",
    "    'cloud_cover': (0.55, 2.79),\n",
    "    'humidity': (0.13, 3.67),\n",
    "    'global_radiation': (-0.64, 2.27)\n",
    "}\n",
    "\n",
    "for feature in ['cloud_cover', 'humidity', 'global_radiation']:\n",
    "    lower_bound, upper_bound = bounds_other[feature]\n",
    "    reshaped_df = cap_outliers(reshaped_df, feature, lower_bound, upper_bound)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1f90049a2f167d0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reshaped_df[['cloud_cover', 'global_radiation', 'humidity', 'precipitation', \n",
    "                'pressure', 'sunshine', 'temp_max', 'temp_mean', 'temp_min', 'wind_speed']].describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57a750e17a711047"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transformation Effectiveness: The transformations and outlier handling techniques have ensured that all features fall within realistic, well-distributed ranges. The skewness and outlier issues have been addressed while retaining important weather patterns."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe5a194afd62b23f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Time Series Data Visualization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1271e8a8cb00e513"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_pivot = reshaped_df.pivot_table(values='temp_mean', index='year', columns='month', aggfunc='mean')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(temp_pivot, annot=True, fmt=\".1f\", cmap='coolwarm')\n",
    "plt.title('Average Temperature by Year and Month')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d81b07ca3b61d73"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reshaped_df['datetime'] = pd.to_datetime(reshaped_df[['year', 'month', 'day']])\n",
    "\n",
    "# Create a dropdown widget for selecting the city\n",
    "city_dropdown = widgets.Dropdown(\n",
    "    options=reshaped_df['city'].unique(),\n",
    "    value=reshaped_df['city'].unique()[0],\n",
    "    description='City:',\n",
    ")\n",
    "\n",
    "# List of features for visualization\n",
    "features = ['sunshine', 'temp_max', 'temp_mean', 'temp_min']\n",
    "\n",
    "def display_plots(city):\n",
    "    city_data = reshaped_df[reshaped_df['city'] == city]\n",
    "    \n",
    "    for feature in features:\n",
    "        fig = px.line(city_data, x='datetime', y=feature, title=f'{feature.replace(\"_\", \" \").title()} Over Time for {city}')\n",
    "        fig.update_layout(xaxis_title='Time', yaxis_title=feature.replace(\"_\", \" \").title(), xaxis_rangeslider_visible=True)\n",
    "        fig.show()\n",
    "\n",
    "widgets.interact(display_plots, city=city_dropdown)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e17d4d75cc4e3952"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multivariate Analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "874cceadbad2d0d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Correlation analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5125594cf6686420"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "numerical_features = ['cloud_cover', 'global_radiation', 'humidity', 'precipitation', \n",
    "                      'pressure', 'sunshine', 'temp_max', 'temp_min', 'wind_speed']\n",
    "numerical_df = reshaped_df[numerical_features + ['temp_mean']]\n",
    "\n",
    "# Compute Pearson and Spearman correlations with the target (temp_mean)\n",
    "pearson_corr_with_target = numerical_df.corr(method='pearson')['temp_mean'].drop('temp_mean')\n",
    "spearman_corr_with_target = numerical_df.corr(method='spearman')['temp_mean'].drop('temp_mean')\n",
    "\n",
    "print(\"Pearson Correlation with Target (temp_mean):\")\n",
    "print(pearson_corr_with_target)\n",
    "\n",
    "print(\"\\nSpearman Correlation with Target (temp_mean):\")\n",
    "print(spearman_corr_with_target)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Pearson Correlation heatmap\n",
    "sns.heatmap(pearson_corr_with_target.to_frame(), annot=True, cmap='coolwarm', fmt='.2f', ax=axes[0], cbar=False)\n",
    "axes[0].set_title('Pearson Correlation with temp_mean')\n",
    "axes[0].set_xticklabels(['temp_mean'], rotation=45, ha='right')\n",
    "axes[0].set_yticklabels(pearson_corr_with_target.index, rotation=0)\n",
    "\n",
    "# Plot Spearman Correlation heatmap\n",
    "sns.heatmap(spearman_corr_with_target.to_frame(), annot=True, cmap='coolwarm', fmt='.2f', ax=axes[1], cbar=False)\n",
    "axes[1].set_title('Spearman Correlation with temp_mean')\n",
    "axes[1].set_xticklabels(['temp_mean'], rotation=45, ha='right')\n",
    "axes[1].set_yticklabels(spearman_corr_with_target.index, rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7567e7195e92f9d1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "numerical_features = ['cloud_cover', 'global_radiation', 'humidity', 'precipitation', \n",
    "                      'pressure', 'sunshine', 'temp_max', 'temp_mean', 'temp_min', \n",
    "                      'wind_speed']\n",
    "numerical_df = reshaped_df[numerical_features]\n",
    "pearson_corr_matrix = numerical_df.corr(method='pearson')\n",
    "spearman_corr_matrix = numerical_df.corr(method='spearman')\n",
    "\n",
    "print(\"Pearson Correlation Matrix:\")\n",
    "print(pearson_corr_matrix)\n",
    "\n",
    "print(\"\\nSpearman Correlation Matrix:\")\n",
    "print(spearman_corr_matrix)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Pearson Correlation Heatmap\n",
    "sns.heatmap(pearson_corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', ax=axes[0])\n",
    "axes[0].set_title('Pearson Correlation Matrix')\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "axes[0].set_yticklabels(axes[0].get_yticklabels(), rotation=45)\n",
    "\n",
    "# Spearman Correlation Heatmap\n",
    "sns.heatmap(spearman_corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', ax=axes[1])\n",
    "axes[1].set_title('Spearman Correlation Matrix')\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "axes[1].set_yticklabels(axes[1].get_yticklabels(), rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d19be831855b097"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pearson Correlation Matrix Analysis (Linear Relationships)\n",
    "\n",
    "cloud_cover and sunshine: Strong negative correlation which makes sense as more cloud cover generally means less sunshine.\n",
    "sunshine and global_radiation : Sunshine hours are strongly related to global radiation, as expected in weather data.\n",
    "humidity and temp_max: Higher temperatures are often associated with lower humidity, which is typical in dry, hot weather conditions.\n",
    "Humidity shows consistent negative correlations with temperature (temp_max and temp_mean) and global radiation, which is typical in dry, sunny weather.\n",
    "\n",
    "Spearman Correlation Matrix Analysis (Rank-Based Relationships)\n",
    "\n",
    "cloud_cover and sunshine: The strong rank-based relationship here implies that more cloud cover is consistently associated with less sunshine.\n",
    "temp_max and global_radiation (0.66): The rank correlation is slightly higher than the Pearson correlation, indicating a strong, possibly non-linear relationship.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c239874020d7d31"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multicollinearity Checks (VIF)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca061ab4266d724a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate VIF for each feature\n",
    "X = reshaped_df[numerical_features]  # Numerical features matrix\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Feature'] = X.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "print(vif_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1d46f3e82d9d5f7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "VIF is a measure of how much the variance of a regression coefficient is inflated due to multicollinearity among the independent variables.\n",
    "\n",
    "VIF > 10: Indicates high multicollinearity\n",
    "VIF > 5: Indicates moderate multicollinearity\n",
    "\n",
    "Temp_max and Temp_mean are two temperature-related features that are highly collinear, likely due to their close relationship."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8f275e01ec78b4c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature Engineering and Selection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a9f40d06ad900dc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reshaped_df['temp_range'] = reshaped_df['temp_max'] - reshaped_df['temp_min']\n",
    "reshaped_df.drop(['temp_max', 'temp_min'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf34a5a21d14f65b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reshaped_df.drop(['pressure', 'cloud_cover'], axis=1, inplace=True)\n",
    "reshaped_df['humidity'] = np.log1p(reshaped_df['humidity'])\n",
    "reshaped_df['global_radiation'] = np.log1p(reshaped_df['global_radiation'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9a0366b5184aa6f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "updated_numerical_features = ['global_radiation', 'humidity', \n",
    "                              'precipitation', 'sunshine', \n",
    "                              'wind_speed', 'temp_range', 'temp_mean']\n",
    "\n",
    "X = reshaped_df[updated_numerical_features]\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Feature'] = X.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "print(vif_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccf3a476c5f46d86"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multivariate Analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1daffa256a070611"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature Importance"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32e1068b94b575b7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the features and target\n",
    "selected_features = ['global_radiation', 'humidity', 'precipitation', 'sunshine', 'wind_speed', 'temp_range']\n",
    "\n",
    "# Prepare data\n",
    "X = reshaped_df[selected_features]\n",
    "y = reshaped_df['temp_mean']\n",
    "\n",
    "# Initialize and fit RandomForestRegressor\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "# Initialize and fit GradientBoostingRegressor\n",
    "gb_model = GradientBoostingRegressor(random_state=42)\n",
    "gb_model.fit(X, y)\n",
    "\n",
    "# Initialize and fit XGBoost Regressor\n",
    "xgb_model = XGBRegressor(random_state=42)\n",
    "xgb_model.fit(X, y)\n",
    "\n",
    "# Extract feature importances from each model\n",
    "rf_importance = pd.Series(rf_model.feature_importances_, index=selected_features)\n",
    "gb_importance = pd.Series(gb_model.feature_importances_, index=selected_features)\n",
    "xgb_importance = pd.Series(xgb_model.feature_importances_, index=selected_features)\n",
    "\n",
    "# Create a DataFrame to compare feature importance across models\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Random Forest': rf_importance,\n",
    "    'Gradient Boosting': gb_importance,\n",
    "    'XGBoost': xgb_importance\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by Random Forest importance for better comparison\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Random Forest', ascending=False)\n",
    "\n",
    "# Print the comparison\n",
    "print(\"Feature Importance Comparison Across Models:\")\n",
    "print(feature_importance_df)\n",
    "\n",
    "# Plot the feature importances\n",
    "import matplotlib.pyplot as plt\n",
    "feature_importance_df.plot(kind='barh', figsize=(10, 6))\n",
    "plt.title('Feature Importance Comparison Across Models')\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb150232220c5fb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reshaped_df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aebda8fd4bd300bc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reshaped_df[selected_features].describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1daf01f1308db7db"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Variable Encoding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e227296351facf6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Cyclic encoding for 'month'\n",
    "reshaped_df['month_sin'] = np.sin(2 * np.pi * reshaped_df['month'] / 12)\n",
    "reshaped_df['month_cos'] = np.cos(2 * np.pi * reshaped_df['month'] / 12)\n",
    "\n",
    "#Cyclic encoding for 'day'\n",
    "reshaped_df['day_sin'] = np.sin(2 * np.pi * reshaped_df['day'] / 31)\n",
    "reshaped_df['day_cos'] = np.cos(2 * np.pi * reshaped_df['day'] / 31)\n",
    "\n",
    "# One-hot encoding for 'city' column\n",
    "reshaped_df = pd.get_dummies(reshaped_df, columns=['city'], drop_first=True)\n",
    "\n",
    "# Drop 'datetime' column\n",
    "reshaped_df.drop(columns=['datetime'], inplace=True)\n",
    "reshaped_df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3dae029209d81ea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modelling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88b7257ee5273811"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to evaluate model performance using cross-validation\n",
    "def evaluate_model_cv(model, X, y, cv_folds=5):\n",
    "    # Define custom scorers for RMSE, MAE, and R-squared\n",
    "    scoring = {\n",
    "        'R-squared': make_scorer(r2_score),\n",
    "        'RMSE': make_scorer(mean_squared_error, squared=False),\n",
    "        'MAE': make_scorer(mean_absolute_error)\n",
    "    }\n",
    "    \n",
    "    # Initialize KFold cross-validation\n",
    "    cv = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Perform cross-validation for each metric\n",
    "    scores = {}\n",
    "    for metric, scorer in scoring.items():\n",
    "        score = cross_val_score(model, X, y, cv=cv, scoring=scorer)\n",
    "        scores[metric] = np.mean(score)  # Average score across folds\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Function to display model comparison and plot line graphs\n",
    "def display_comparison(results):\n",
    "    results_df = pd.DataFrame(results).T  # Transpose to get models as rows\n",
    "    print(results_df)\n",
    "\n",
    "    # Plot the comparison using line graphs for each metric\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # R-squared Plot\n",
    "    results_df[['R-squared']].plot(ax=ax[0], marker='o', linestyle='-', color='blue')\n",
    "    ax[0].set_title(\"R-squared Comparison\")\n",
    "    ax[0].set_ylabel(\"R-squared\")\n",
    "    ax[0].set_xlabel(\"Models\")\n",
    "    ax[0].set_xticks(range(len(results_df.index)))\n",
    "    ax[0].set_xticklabels(results_df.index, rotation=45)\n",
    "    \n",
    "    # RMSE Plot\n",
    "    results_df[['RMSE']].plot(ax=ax[1], marker='o', linestyle='-', color='red')\n",
    "    ax[1].set_title(\"RMSE Comparison\")\n",
    "    ax[1].set_ylabel(\"RMSE\")\n",
    "    ax[1].set_xlabel(\"Models\")\n",
    "    ax[1].set_xticks(range(len(results_df.index)))\n",
    "    ax[1].set_xticklabels(results_df.index, rotation=45)\n",
    "    \n",
    "    # MAE Plot\n",
    "    results_df[['MAE']].plot(ax=ax[2], marker='o', linestyle='-', color='green')\n",
    "    ax[2].set_title(\"MAE Comparison\")\n",
    "    ax[2].set_ylabel(\"MAE\")\n",
    "    ax[2].set_xlabel(\"Models\")\n",
    "    ax[2].set_xticks(range(len(results_df.index)))\n",
    "    ax[2].set_xticklabels(results_df.index, rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main function to run the comparison across multiple regressors with cross-validation\n",
    "def compare_regressors_cv(X, y, cv_folds=5):\n",
    "    # Initialize models\n",
    "    models = {\n",
    "        'Elastic Net': ElasticNet(random_state=42),\n",
    "        'Random Forest': RandomForestRegressor(random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "        'XGBoost': XGBRegressor(random_state=42),\n",
    "        'LightGBM': LGBMRegressor(random_state=42),\n",
    "        'CatBoost': CatBoostRegressor(random_state=42, verbose=0),\n",
    "        'AdaBoost': AdaBoostRegressor(random_state=42)\n",
    "    }\n",
    "\n",
    "    # Dictionary to store results\n",
    "    results = {}\n",
    "    \n",
    "    # Loop over models and store their performance\n",
    "    for name, model in models.items():\n",
    "        results[name] = evaluate_model_cv(model, X, y, cv_folds=cv_folds)\n",
    "    \n",
    "    # Display comparison of models\n",
    "    display_comparison(results)\n",
    "\n",
    "# Usage\n",
    "# Assuming you already have reshaped_df, selected_features, and temp_mean as target\n",
    "X = reshaped_df.drop(columns=['temp_mean'])  # Features\n",
    "y = reshaped_df['temp_mean']  # Target variable\n",
    "\n",
    "# Compare regressors with 5-fold cross-validation\n",
    "compare_regressors_cv(X, y, cv_folds=5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8c895ec422b297a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import optuna\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Objective function for CatBoost\n",
    "def objective_catboost(trial):\n",
    "    param = {\n",
    "        'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "        'depth': trial.suggest_int('depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
    "        'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-8, 10.0),\n",
    "        'bagging_temperature': trial.suggest_uniform('bagging_temperature', 0.0, 1.0),\n",
    "        'random_strength': trial.suggest_uniform('random_strength', 0.0, 1.0)\n",
    "    }\n",
    "\n",
    "    model = CatBoostRegressor(**param, random_state=42, verbose=0)\n",
    "    \n",
    "    # Perform a train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "# Create the study for CatBoost\n",
    "study_catboost = optuna.create_study(direction='minimize')\n",
    "study_catboost.optimize(objective_catboost, n_trials=50)\n",
    "\n",
    "# Print the best hyperparameters and RMSE for CatBoost\n",
    "print(\"Best hyperparameters for CatBoost:\", study_catboost.best_trial.params)\n",
    "print(\"Best RMSE for CatBoost:\", study_catboost.best_trial.value)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b04ad0dd1a06f1d9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Objective function for RandomForestRegressor\n",
    "def objective_rf(trial):\n",
    "    # Define the hyperparameters to tune\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 1000)\n",
    "    max_depth = trial.suggest_int('max_depth', 5, 30)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 4)\n",
    "    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])  # Replaced 'auto' with 'sqrt'\n",
    "    \n",
    "    # Initialize the RandomForestRegressor with trial hyperparameters\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Perform a train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "# Create the Optuna study to minimize the RMSE\n",
    "study_rf = optuna.create_study(direction='minimize')\n",
    "study_rf.optimize(objective_rf, n_trials=50)\n",
    "\n",
    "# Print the best hyperparameters and RMSE\n",
    "print(\"Best hyperparameters for Random Forest:\", study_rf.best_trial.params)\n",
    "print(\"Best RMSE for Random Forest:\", study_rf.best_trial.value)\n",
    "\n",
    "# Create the Optuna study to minimize the RMSE\n",
    "study_rf = optuna.create_study(direction='minimize')\n",
    "study_rf.optimize(objective_rf, n_trials=50)\n",
    "\n",
    "# Print the best hyperparameters and RMSE\n",
    "print(\"Best hyperparameters for Random Forest:\", study_rf.best_trial.params)\n",
    "print(\"Best RMSE for Random Forest:\", study_rf.best_trial.value)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9dfdb30440b3b248"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Objective function for XGBoost\n",
    "def objective_xgb(trial):\n",
    "    param = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0)\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBRegressor(**param, random_state=42)\n",
    "    \n",
    "    # Perform a train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "# Create the study for XGBoost\n",
    "study_xgb = optuna.create_study(direction='minimize')\n",
    "study_xgb.optimize(objective_xgb, n_trials=50)\n",
    "\n",
    "# Print the best hyperparameters and RMSE for XGBoost\n",
    "print(\"Best hyperparameters for XGBoost:\", study_xgb.best_trial.params)\n",
    "print(\"Best RMSE for XGBoost:\", study_xgb.best_trial.value)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5f4f806e82518c1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Final hyperparameters from Optuna\n",
    "best_params = {\n",
    "    'n_estimators': 984,\n",
    "    'max_depth': 14,\n",
    "    'learning_rate': 0.017,\n",
    "    'subsample': 0.815,\n",
    "    'colsample_bytree': 0.972,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Initialize XGBoost model with best hyperparameters\n",
    "xgb_model = xgb.XGBRegressor(**best_params)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Final Evaluation of XGBoost:\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "# Plot true vs predicted values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5, color=\"blue\")\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color=\"red\", lw=2)\n",
    "plt.title(\"XGBoost: True vs Predicted Values\")\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f1872f940cde35b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "\n",
    "# Grouping by year and month, and calculating the mean for each group\n",
    "grouped_test = X_test.groupby(['year', 'month'], as_index=False).mean()\n",
    "\n",
    "# Select the city for which you want to visualize the data\n",
    "city_to_plot = 'SONNBLICK'\n",
    "city_test_mask = X_test['city_' + city_to_plot] == 1  # Mask to filter test data for this city\n",
    "\n",
    "# Group y_test and y_pred by year and month similarly\n",
    "y_test_grouped = pd.DataFrame({'Date': X_test[city_test_mask]['datetime'], 'mean_temp': y_test[city_test_mask]})\n",
    "y_pred_grouped = pd.DataFrame({'Date': X_test[city_test_mask]['datetime'], 'mean_temp': y_pred[city_test_mask]})\n",
    "\n",
    "# Now group them by year and month, taking mean temperature for both true and predicted values\n",
    "y_test_grouped = y_test_grouped.groupby([y_test_grouped['Date'].dt.year, y_test_grouped['Date'].dt.month], as_index=False).mean()\n",
    "y_pred_grouped = y_pred_grouped.groupby([y_pred_grouped['Date'].dt.year, y_pred_grouped['Date'].dt.month], as_index=False).mean()\n",
    "\n",
    "# Plotly subplots\n",
    "fig = make_subplots(rows=2, cols=1)\n",
    "\n",
    "# Actual mean_temp (Truth)\n",
    "fig.add_trace(go.Scatter(x=y_test_grouped['Date'], y=y_test_grouped['mean_temp'],\n",
    "                         name='True Mean Temp', marker_color='LightSkyBlue'), row=1, col=1)\n",
    "\n",
    "# Predicted mean_temp (Prediction)\n",
    "fig.add_trace(go.Scatter(x=y_pred_grouped['Date'], y=y_pred_grouped['mean_temp'],\n",
    "                         name='Predicted Mean Temp', marker_color='MediumPurple'), row=1, col=1)\n",
    "\n",
    "# Plot on the second subplot for future predictions and comparison\n",
    "fig.add_trace(go.Scatter(x=y_test_grouped['Date'], y=y_test_grouped['mean_temp'],\n",
    "                         name='True Mean Temp (Future)', marker_color='LightSkyBlue', showlegend=False), row=2, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=y_pred_grouped['Date'], y=y_pred_grouped['mean_temp'],\n",
    "                         name='Predicted Mean Temp (Future)', marker_color='MediumPurple', showlegend=False), row=2, col=1)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title=f\"True vs Predicted Mean Temperature for {city_to_plot}\",\n",
    "                  xaxis_title=\"Date\",\n",
    "                  yaxis_title=\"Mean Temperature\",\n",
    "                  height=600, width=1000)\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71fc0ef6d15c94ea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reshaped_df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab5799ca830d427f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torch.nn.utils.spectral_norm as spectral_norm\n",
    "\n",
    "# Define weather features and number of features\n",
    "weather_features = ['global_radiation', 'humidity', 'precipitation', 'sunshine', 'wind_speed', 'temp_range']\n",
    "n_features = len(weather_features)\n",
    "\n",
    "# -------------------- 1. Network Definitions --------------------\n",
    "\n",
    "# Generator network for multivariate time-series\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, seq_len, n_features):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, seq_len * n_features),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.model(z)\n",
    "        x = x.view(-1, seq_len, n_features)\n",
    "        return x\n",
    "\n",
    "# Discriminator network for multivariate time-series with Spectral Normalization\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, seq_len, n_features):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            spectral_norm(nn.Linear(seq_len * n_features, 512)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            spectral_norm(nn.Linear(512, 256)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            spectral_norm(nn.Linear(256, 1))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, seq_len * n_features)\n",
    "        return self.model(x)\n",
    "\n",
    "# -------------------- 2. Utility Functions --------------------\n",
    "\n",
    "# Apply small Gaussian noise to discriminator inputs\n",
    "def add_noise_to_data(data, noise_std=0.1):\n",
    "    noise = noise_std * torch.randn_like(data)\n",
    "    return data + noise\n",
    "\n",
    "# LSGAN loss for Discriminator\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = torch.mean((real_output - 1)**2)  # Least squares loss\n",
    "    fake_loss = torch.mean(fake_output**2)\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "# LSGAN loss for Generator\n",
    "def generator_loss(fake_output):\n",
    "    return torch.mean((fake_output - 1)**2)\n",
    "\n",
    "# Data preprocessing using StandardScaler\n",
    "def preprocess_data(data):\n",
    "    data_scaler = StandardScaler()\n",
    "    data = data.to_numpy()\n",
    "    n_rows = data.shape[0]\n",
    "    n_features = data.shape[1]\n",
    "    truncated_rows = (n_rows // 24) * 24\n",
    "    data = data[:truncated_rows, :]\n",
    "    data = data_scaler.fit_transform(data.reshape(-1, n_features))\n",
    "    data = data.reshape(-1, 24, n_features)\n",
    "    return data, data_scaler\n",
    "\n",
    "# Generate real data batches\n",
    "def real_data_gen(weather_data, batch_size):\n",
    "    idx = np.random.randint(0, len(weather_data) - batch_size)\n",
    "    return torch.tensor(weather_data[idx:idx + batch_size], dtype=torch.float32)\n",
    "\n",
    "# Generate random noise for the generator input\n",
    "def generate_noise(batch_size, latent_dim):\n",
    "    return torch.randn(batch_size, latent_dim)\n",
    "\n",
    "# Function to train the discriminator\n",
    "def train_discriminator(discriminator, optimizer, real_data, fake_data):\n",
    "    optimizer.zero_grad()\n",
    "    fake_data_detached = fake_data.detach()\n",
    "\n",
    "    real_data_noisy = add_noise_to_data(real_data)\n",
    "    fake_data_noisy = add_noise_to_data(fake_data_detached)\n",
    "\n",
    "    real_output = discriminator(real_data_noisy)\n",
    "    fake_output = discriminator(fake_data_noisy)\n",
    "    \n",
    "    d_loss = discriminator_loss(real_output, fake_output)\n",
    "    return d_loss\n",
    "\n",
    "# Function to train the generator\n",
    "def train_generator(generator, discriminator, optimizer, fake_data):\n",
    "    optimizer.zero_grad()\n",
    "    prediction = discriminator(fake_data)\n",
    "    g_loss = generator_loss(prediction)\n",
    "    g_loss.backward()\n",
    "    optimizer.step()\n",
    "    return g_loss\n",
    "\n",
    "# Visualize data with t-SNE\n",
    "def visualize_tsne(real_data, synthetic_data):\n",
    "    tsne = TSNE(n_components=2, perplexity=30)\n",
    "    real_embedded = tsne.fit_transform(real_data)\n",
    "    synthetic_embedded = tsne.fit_transform(synthetic_data)\n",
    "    \n",
    "    plt.scatter(real_embedded[:, 0], real_embedded[:, 1], label='Real', alpha=0.5)\n",
    "    plt.scatter(synthetic_embedded[:, 0], real_embedded[:, 1], label='Synthetic', alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.title('t-SNE comparison of Real and Synthetic Data')\n",
    "    plt.show()\n",
    "\n",
    "# -------------------- 3. Training Setup --------------------\n",
    "\n",
    "# Hyperparameters\n",
    "seq_len = 24\n",
    "latent_dim = 128\n",
    "batch_size = 64\n",
    "learning_rate_D = 5e-5  # Learning rate adjusted for LSGAN stability\n",
    "learning_rate_G = 2e-4\n",
    "num_epochs = 10000\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the generator and discriminator\n",
    "generator = Generator(latent_dim, seq_len, n_features).to(device)\n",
    "discriminator = Discriminator(seq_len, n_features).to(device)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate_D, betas=(0.5, 0.999))\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate_G, betas=(0.5, 0.999))\n",
    "\n",
    "# Load and preprocess weather data\n",
    "weather_data = reshaped_df[weather_features]\n",
    "weather_data, data_scaler = preprocess_data(weather_data)\n",
    "\n",
    "# -------------------- 4. Training Loop --------------------\n",
    "\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    real_data = real_data_gen(weather_data, batch_size).to(device)\n",
    "\n",
    "    # Generate new noise for each discriminator step\n",
    "    noise = generate_noise(batch_size, latent_dim).to(device)\n",
    "    fake_data = generator(noise)\n",
    "\n",
    "    # Train the discriminator less frequently\n",
    "    if epoch % 10 == 0:\n",
    "        optimizer_D.zero_grad()\n",
    "        d_error = train_discriminator(discriminator, optimizer_D, real_data, fake_data)\n",
    "        d_error.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "    # Train the generator more frequently\n",
    "    optimizer_G.zero_grad()\n",
    "\n",
    "    noise = generate_noise(batch_size, latent_dim).to(device)\n",
    "    fake_data = generator(noise)\n",
    "\n",
    "    g_error = train_generator(generator, discriminator, optimizer_G, fake_data)\n",
    "\n",
    "    # Store losses\n",
    "    d_losses.append(d_error.item() if 'd_error' in locals() else 0)\n",
    "    g_losses.append(g_error.item())\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Epoch {epoch} | D Loss: {d_error.item():.4f} | G Loss: {g_error.item():.4f}')\n",
    "        \n",
    "        # Visualize intermediate results with t-SNE\n",
    "        with torch.no_grad():\n",
    "            synthetic_data = generator(generate_noise(len(weather_data), latent_dim).to(device)).cpu().numpy().reshape(-1, n_features)\n",
    "            visualize_tsne(weather_data.reshape(-1, n_features), synthetic_data)\n",
    "\n",
    "# Plot the losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(d_losses, label='Discriminator Loss')\n",
    "plt.plot(g_losses, label='Generator Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss Curves')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1305ab10891d6d57"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_histograms(real_data, synthetic_data, feature_names):\n",
    "    # Get the number of features based on the real data\n",
    "    num_features = min(real_data.shape[1], synthetic_data.shape[1])  # Ensure we don't exceed the smallest dimension\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i in range(num_features):\n",
    "        plt.subplot(int(np.ceil(num_features / 3)), 3, i + 1)\n",
    "        \n",
    "        # Plot real data histogram\n",
    "        plt.hist(real_data[:, i], bins=20, alpha=0.6, label='Real', density=True)\n",
    "        \n",
    "        # Plot synthetic data histogram\n",
    "        plt.hist(synthetic_data[:, i], bins=20, alpha=0.6, label='Synthetic', density=True)\n",
    "        \n",
    "        # Ensure the title corresponds to the correct feature\n",
    "        plt.title(feature_names[i] if i < len(feature_names) else f\"Feature {i}\")\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming `weather_data` is a Pandas DataFrame and `synthetic_weather_data` is a NumPy array.\n",
    "# Convert the real weather data (Pandas DataFrame) to a NumPy array\n",
    "real_data = weather_data\n",
    "\n",
    "# Ensure synthetic_weather_data is already a NumPy array (if not, convert it similarly)\n",
    "# For demonstration purposes, I assume it is already a NumPy array.\n",
    "\n",
    "# Plot the histograms\n",
    "plot_histograms(real_data, synthetic_weather_data, weather_features)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7225831646372934"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "synthetic_weather_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1365a614d319c12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4f5af785df110b62"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
